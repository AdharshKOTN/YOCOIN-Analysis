---
title: "Analysis on YOCOIN Data"
author: "Abhisek Banerjee, Nirmohi Dave"
date: '`r format(Sys.time(), "%B %d, %Y")`'
output: html_document
---
Introduction
------------
The YoCoIN analysis project aims to analyze data of the Etherium coin YOCOIN and find out if we can deduce anything significant from the available data.

What is Etherium?
-----------------
"Ethereum is a decentralized platform that runs smart contracts: applications that run exactly as programmed without any possibility of downtime, censorship, fraud or third-party interference. These apps run on a custom built blockchain, an enormously powerful shared global infrastructure that can move value around and represent the ownership of property."- [Source : https://www.ethereum.org/]

What is ERC20?
--------------
ERC20 is a significant standard for tokens on Etherium. This feines a common list of rules that enables developers to  to accurately predict how new tokens will function within the larger Ethereum system.
Source[https://www.investopedia.com/news/what-erc20-and-what-does-it-mean-ethereum/]

YOCOIN TOKEN
------------
YOcoin was founded & launched on December 7th, 2015. It is an open-ledger, publicly exchanged, peer-to-peer crypto currency that is designed for the general public worldwide and will be utilized to pay for goods and services by many different industries across the globe, including but not limited to the direct sales industry. It uses the Etherium network for transaction and storage. [Sources:
https://yocoinweb.wordpress.com/, 
https://steemit.com/yocoin/@tonypeacock/things-you-should-know-about-yocoin]

We have analysed transaction data of YOCOIN over specific periof of time.YOCOIN has 10 decimal points .i.e. 10^16 tokens make a single YOCOIN and also there were 310,000,000 YOCOINs available at the time of our analysis.

Our goal
--------
We tried to achieve couple of things during this project.

1> We have taken the sellers and buyers information out of the dataset and tried to plot their frequencies to find out what distribution they follow.

2> We have taken number of transactions for different dates between 07-21-2016 and  02-05-2018, and the corresponding highest token prices for each day and then tried to find out any correlation between highest stock price and number of transaction in a particular day. To do this we split our dataset into multiple layers(bins) and computed correlation value for each layers.

Preprocessing
-------------
Before starting the analysis, we have done some preprosessing on the data and removed few outliers.

1> We removed all the transections which were dealing with coins more than the total available coins.These were spurious transactiions and we do not need to consider these for our project.

2> Then we removed very big transactions and very small transactions. These were extreme outliers and removing these values yielded better results.

Packages Used
-------------
We used the following packages in our code.
1>'fitdistrplus': This has been used to plot sellers and buyers data in different distributions.
2>'ggplot2': This has been used to plot data along different axes with different attributes.
3> 'reshape' : This has been used to join tables.

Analysis
--------
```{r setup, include=FALSE, comment=NA}
knitr::opts_chunk$set(echo = FALSE)
```
```{r message=FALSE}
library(fitdistrplus)
library(ggplot2)
library(reshape)

```



```{r comment=NA, YoCoin}

unprocessed_data<-read.csv(file="networkyocoinTX.txt", header = F, sep=" ")
colnames(unprocessed_data)<-c("Sellers", "Buyers", "TimeStamps", "TokenAmounts")

message("Number of rows in unprocessed data: ", nrow(unprocessed_data))
message("Summary of the unprocessed data:")
summary(unprocessed_data)
total_circulation <-(31.000000e+23)
outliers<-subset(unprocessed_data, TokenAmounts > total_circulation)
message("Number of outlier rows: ", nrow(outliers))
message("Summary of the outliers")
summary(outliers)
message("Below is the table of outlying buyers and the frequencies of trnsactions.")
outlying_buyers<-c(outliers$Buyers)
outlying_buyers_table<-as.data.frame(table(outlying_buyers))
outlying_buyers_table
message("Below is the table of outlying sellers and the frequencies of transactions.")
outlying_sellers<-c(outliers$Sellers)
outlier_sellers_table<-as.data.frame(table(outlying_sellers))
outlier_sellers_table

print("First we would remove impossible transaction.")
data_preprocessed<-subset(unprocessed_data, TokenAmounts <= total_circulation)
summary(data_preprocessed)

print("We would do one more round of pre-processing. We will remove 1 percentile of data at both the sides.")
data<-data_preprocessed[data_preprocessed$TokenAmounts >                                                            quantile(data_preprocessed$TokenAmounts, 0.01), ]
data<-data[data$TokenAmounts < quantile(data_preprocessed$TokenAmounts, 0.99), ]
message("Number of rows in processed data: ", nrow(data))
message("Summary of the processed data:")
summary(data)
Buyers<-c(data$Buyers)
Sellers<-c(data$Sellers)
SellersTable<-as.data.frame(table(Sellers))
BuyersTable<-as.data.frame(table(Buyers))
SellersFreq<-(as.data.frame(table(SellersTable[,'Freq'])))
BuyersFreq<-(as.data.frame(table(BuyersTable[,'Freq'])))
seller_frequency <-c(SellersTable[,'Freq'])
message("Summary of seller frequencies:")
summary(seller_frequency)
buyers_frequency <-c(BuyersTable[,'Freq'])
message("Summary of buyer frequencies:")
summary(buyers_frequency)
seller_freq_frequency <-c(SellersFreq[,'Freq'])
message("Summary of frequencies of seller frequencies:")
summary(seller_freq_frequency)
buyers_freq_frequency <-c(BuyersFreq[,'Freq'])
message("Summary of frequencies of buyer frequencies:")
summary(buyers_freq_frequency)
```
After analysing the data, we have made an assumption that all these frequency data follow exponential distributions.We will try to plot exponential curves for all these data and then check them with Cullen and Frey graphs to verify their accuracy. If there is a mismatch, we will try to plot for the new type of distribution as well.

```{r comment=NA, Plot}
message("Plots for frequencies of sellers:")
normal_dist_sellers <- fitdist(seller_frequency, "exp")
plot(normal_dist_sellers)
descdist(seller_frequency, discrete = TRUE)
message("After drawing the Cullen and Frey graph, this looks somewhat close to poisson distribution.")
normal_dist_sellers <- fitdist(seller_frequency, "pois")
plot(normal_dist_sellers)

message("Plots for frequencies of buyers:")
normal_dist_buyers <- fitdist(buyers_frequency, "exp")
plot(normal_dist_buyers)
descdist(buyers_frequency, discrete = TRUE)
message("After drawing the Cullen and Frey graph, this looks somewhat close to negative binomial distribution.")
normal_dist_buyers <- fitdist(buyers_frequency, "nbinom")
plot(normal_dist_buyers)


message("Plots for frequencies of frequencies of sellers:")
normal_dist_sellers_freq <- fitdist(seller_freq_frequency, "exp")
plot(normal_dist_sellers_freq)
descdist(seller_freq_frequency, discrete = TRUE)
message("After drawing the Cullen and Frey graph, this looks somewhat close to negative binomial distribution.")
normal_dist_sellers_freq <- fitdist(seller_freq_frequency, "nbinom")
plot(normal_dist_sellers_freq)

message("Plots for frequencies of frequencies of buyers:")
normal_dist_buyers_freq <- fitdist(seller_freq_frequency, "exp")
plot(normal_dist_buyers_freq)
descdist(buyers_freq_frequency, discrete = TRUE)
message("After drawing the Cullen and Frey graph, this looks somewhat close to negative binomial distribution.")
normal_dist_buyers_freq <- fitdist(seller_freq_frequency, "nbinom")
plot(normal_dist_buyers_freq)
```

```{r comment=NA, Correlation}

print("Now we will create multiple layers from the data and merge them with the highest price for each day. ")
max_amount=max(data$TokenAmounts)
this_layer_data = data
previous_layer_res_data = data
multiplyer = 1
price_table<-read.csv(file="yocoin.txt", header = T, sep="\t")
colnames(price_table)<-c("Date", "Open", "High", "Low", "Close", "Volume", "Market_Cap")
price_date<-c(price_table$Date)
price_high<-c(price_table$High)
column_to_keep<-c("Date", "High")
price_and_date<-price_table[column_to_keep]
price_and_date$Date<-gsub("/","-",price_and_date$Date)
price_and_date$Date<-format(as.Date(price_and_date$Date, format="%m-%d-%Y"), "%Y-%m-%d")
max_trx = max_amount
  factor = 1
  #Layer1
  message("Layer", factor)
  multiplyer = multiplyer/10
  message("Minimum trx val= " , max_amount*multiplyer, " Maximum trx val = ", max_trx)
  this_layer_data<-subset(previous_layer_res_data, previous_layer_res_data$TokenAmounts >= (max_amount*multiplyer))
  previous_layer_res_data<-subset(previous_layer_res_data,                                               previous_layer_res_data$TokenAmounts < (max_amount*multiplyer))
#print(ggplot(this_layer_data,aes(y = this_layer_data$TokenAmounts, x = this_layer_data$TimeStamps)) +geom_point())
 this_layer_data$TimeStamps <-as.Date(as.POSIXct(as.numeric(this_layer_data$TimeStamps), origin="1970-01-01"))
 colnames(this_layer_data)[which(colnames(this_layer_data) == 'TimeStamps')] <- 'Date'
  Date<-c(this_layer_data$Date)
  Date<-as.data.frame(table(Date))
  merged_table<-merge(price_and_date, Date, all=FALSE)
  message("number of rows in this layer :", nrow(this_layer_data))
  message("number of rows once this layer is removed :", nrow(previous_layer_res_data))
  print(cor.test(merged_table$High,merged_table$Freq, method = "pearson"))
  cor_plot <- melt(merged_table, id.vars="Date") 
  print(ggplot(cor_plot, aes(Date,value, col=variable)) + geom_point() + stat_smooth())
  factor = factor + 1

  #Layer 2
  message("Layer", factor)
  multiplyer = multiplyer/10
  message("Minimum trx val= " , max_amount*multiplyer, " Maximum trx val = ", max_trx)
  this_layer_data<-subset(previous_layer_res_data, previous_layer_res_data$TokenAmounts >= (max_amount*multiplyer))
  previous_layer_res_data<-subset(previous_layer_res_data,                                               previous_layer_res_data$TokenAmounts < (max_amount*multiplyer))
#print(ggplot(this_layer_data,aes(y = this_layer_data$TokenAmounts, x = this_layer_data$TimeStamps)) +geom_point())
 this_layer_data$TimeStamps <-as.Date(as.POSIXct(as.numeric(this_layer_data$TimeStamps), origin="1970-01-01"))
 colnames(this_layer_data)[which(colnames(this_layer_data) == 'TimeStamps')] <- 'Date'
  Date<-c(this_layer_data$Date)
  Date<-as.data.frame(table(Date))
  merged_table<-merge(price_and_date, Date, all=FALSE)
  message("number of rows in this layer :", nrow(this_layer_data))
  message("number of rows once this layer is removed :", nrow(previous_layer_res_data))
  print(cor.test(merged_table$High,merged_table$Freq, method = "pearson"))
  cor_plot <- melt(merged_table, id.vars="Date") 
  print(ggplot(cor_plot, aes(Date,value, col=variable)) + geom_point() + stat_smooth())
  factor = factor + 1
  
  #Layer3
  message("Layer", factor)
  multiplyer = multiplyer/10
  message("Minimum trx val= " , max_amount*multiplyer, " Maximum trx val = ", max_trx)
  this_layer_data<-subset(previous_layer_res_data, previous_layer_res_data$TokenAmounts >= (max_amount*multiplyer))
  previous_layer_res_data<-subset(previous_layer_res_data,                                               previous_layer_res_data$TokenAmounts < (max_amount*multiplyer))
#print(ggplot(this_layer_data,aes(y = this_layer_data$TokenAmounts, x = this_layer_data$TimeStamps)) +geom_point())
 this_layer_data$TimeStamps <-as.Date(as.POSIXct(as.numeric(this_layer_data$TimeStamps), origin="1970-01-01"))
 colnames(this_layer_data)[which(colnames(this_layer_data) == 'TimeStamps')] <- 'Date'
  Date<-c(this_layer_data$Date)
  Date<-as.data.frame(table(Date))
  merged_table<-merge(price_and_date, Date, all=FALSE)
  message("number of rows in this layer :", nrow(this_layer_data))
  message("number of rows once this layer is removed :", nrow(previous_layer_res_data))
  print(cor.test(merged_table$High,merged_table$Freq, method = "pearson"))
  cor_plot <- melt(merged_table, id.vars="Date")
  print(ggplot(cor_plot, aes(Date,value, col=variable)) + geom_point() + stat_smooth())
  factor = factor + 1
  
  #Layer4
  message("Layer", factor)
  multiplyer = multiplyer/10
  message("Minimum trx val= " , max_amount*multiplyer, " Maximum trx val = ", max_trx)
  this_layer_data<-subset(previous_layer_res_data, previous_layer_res_data$TokenAmounts >= (max_amount*multiplyer))
  previous_layer_res_data<-subset(previous_layer_res_data,                                               previous_layer_res_data$TokenAmounts < (max_amount*multiplyer))
#print(ggplot(this_layer_data,aes(y = this_layer_data$TokenAmounts, x = this_layer_data$TimeStamps)) +geom_point())
 this_layer_data$TimeStamps <-as.Date(as.POSIXct(as.numeric(this_layer_data$TimeStamps), origin="1970-01-01"))
 colnames(this_layer_data)[which(colnames(this_layer_data) == 'TimeStamps')] <- 'Date'
  Date<-c(this_layer_data$Date)
  Date<-as.data.frame(table(Date))
  merged_table<-merge(price_and_date, Date, all=FALSE)
  message("number of rows in this layer :", nrow(this_layer_data))
  message("number of rows once this layer is removed :", nrow(previous_layer_res_data))
  print(cor.test(merged_table$High,merged_table$Freq, method = "pearson"))
  cor_plot <- melt(merged_table, id.vars="Date") 
  print(ggplot(cor_plot, aes(Date,value, col=variable)) + geom_point() + stat_smooth())
  factor = factor + 1
  
  #Layer5
  message("Layer", factor)
  multiplyer = multiplyer/10
  message("Minimum trx val= " , max_amount*multiplyer, " Maximum trx val = ", max_trx)
  this_layer_data<-subset(previous_layer_res_data, previous_layer_res_data$TokenAmounts >= (max_amount*multiplyer))
  previous_layer_res_data<-subset(previous_layer_res_data,                                               previous_layer_res_data$TokenAmounts < (max_amount*multiplyer))
#print(ggplot(this_layer_data,aes(y = this_layer_data$TokenAmounts, x = this_layer_data$TimeStamps)) +geom_point())
 this_layer_data$TimeStamps <-as.Date(as.POSIXct(as.numeric(this_layer_data$TimeStamps), origin="1970-01-01"))
 colnames(this_layer_data)[which(colnames(this_layer_data) == 'TimeStamps')] <- 'Date'
  Date<-c(this_layer_data$Date)
  Date<-as.data.frame(table(Date))
  merged_table<-merge(price_and_date, Date, all=FALSE)
  message("number of rows in this layer :", nrow(this_layer_data))
  message("number of rows once this layer is removed :", nrow(previous_layer_res_data))
  print(cor.test(merged_table$High,merged_table$Freq, method = "pearson"))
  cor_plot <- melt(merged_table, id.vars="Date") 
  print(ggplot(cor_plot, aes(Date,value, col=variable)) + geom_point() + stat_smooth())
  factor = factor + 1

#Layer6
  message("Layer", factor)
  multiplyer = multiplyer/10
  message("Minimum trx val= " , max_amount*multiplyer, " Maximum trx val = ", max_trx)
  this_layer_data<-subset(previous_layer_res_data, previous_layer_res_data$TokenAmounts >= (max_amount*multiplyer))
  previous_layer_res_data<-subset(previous_layer_res_data,                                               previous_layer_res_data$TokenAmounts < (max_amount*multiplyer))
#print(ggplot(this_layer_data,aes(y = this_layer_data$TokenAmounts, x = this_layer_data$TimeStamps)) +geom_point())
 this_layer_data$TimeStamps <-as.Date(as.POSIXct(as.numeric(this_layer_data$TimeStamps), origin="1970-01-01"))
 colnames(this_layer_data)[which(colnames(this_layer_data) == 'TimeStamps')] <- 'Date'
  Date<-c(this_layer_data$Date)
  Date<-as.data.frame(table(Date))
  merged_table<-merge(price_and_date, Date, all=FALSE)
  message("number of rows in this layer :", nrow(this_layer_data))
  message("number of rows once this layer is removed :", nrow(previous_layer_res_data))
  print(cor.test(merged_table$High,merged_table$Freq, method = "pearson"))
  cor_plot <- melt(merged_table, id.vars="Date") 
  print(ggplot(cor_plot, aes(Date,value, col=variable)) + geom_point() + stat_smooth())
  factor = factor + 1
  
  #Layer7
  message("Layer", factor)
  multiplyer = multiplyer/10
  message("Minimum trx val= " , max_amount*multiplyer, " Maximum trx val = ", max_trx)
  this_layer_data<-subset(previous_layer_res_data, previous_layer_res_data$TokenAmounts >= (max_amount*multiplyer))
  previous_layer_res_data<-subset(previous_layer_res_data,                                               previous_layer_res_data$TokenAmounts < (max_amount*multiplyer))
#print(ggplot(this_layer_data,aes(y = this_layer_data$TokenAmounts, x = this_layer_data$TimeStamps)) +geom_point())
 this_layer_data$TimeStamps <-as.Date(as.POSIXct(as.numeric(this_layer_data$TimeStamps), origin="1970-01-01"))
 colnames(this_layer_data)[which(colnames(this_layer_data) == 'TimeStamps')] <- 'Date'
  Date<-c(this_layer_data$Date)
  Date<-as.data.frame(table(Date))
  merged_table<-merge(price_and_date, Date, all=FALSE)
  message("number of rows in this layer :", nrow(this_layer_data))
  message("number of rows once this layer is removed :", nrow(previous_layer_res_data))
  print(cor.test(merged_table$High,merged_table$Freq, method = "pearson"))
  cor_plot <- melt(merged_table, id.vars="Date") 
  print(ggplot(cor_plot, aes(Date,value, col=variable)) + geom_point() + stat_smooth())
  factor = factor + 1
  
#message("Number of data point in the last layer is ", nrow(previous_layer_res_data),". This is too less and we will exclude this.")

#message("number of rows in this layer :",nrow(previous_layer_res_data))
#previous_layer_res_data$TimeStamps<-as.Date(as.POSIXct(as.numeric(previous_layer_res_data$TimeStamps), origin="1970-01-01"))
 #colnames(previous_layer_res_data)[which(colnames(previous_layer_res_data) == 'TimeStamps')] <- 'Date'
  #Date<-c(previous_layer_res_data$Date)
  #Date<-as.data.frame(table(Date))
  #merged_table<-merge(price_and_date, Date, all=FALSE)

  #print(cor.test(merged_table$High,merged_table$Freq, method = "pearson"))

  
#cor_plot <- melt(merged_table, id.vars="Date") 
#ggplot(cor_plot, aes(Date,value, col=variable)) +geom_point() + stat_smooth()

message("Let's take correlation for the entire data set")
this_layer_data<-data
this_layer_data$TimeStamps <-as.Date(as.POSIXct(as.numeric(this_layer_data$TimeStamps), origin="1970-01-01"))
colnames(this_layer_data)[which(colnames(this_layer_data) == 'TimeStamps')] <- 'Date'
Date<-c(this_layer_data$Date)
Date<-as.data.frame(table(Date))
merged_table<-merge(price_and_date, Date, all=FALSE)
cor.test(merged_table$High,merged_table$Freq, method = "pearson")
 

cor_plot <- melt(merged_table, id.vars="Date") 
ggplot(cor_plot, aes(Date,value, col=variable)) + geom_point() + stat_smooth()  
 
 message("Layer 4 had the highest correlation of 0.1811658. But the correlation values for different layers are very small.")

 
```
Conclusion
----------
After analyzing the YOCOIN data we can conclude, that there is a very less dependence of coin price with respect to the vumber of transactions. Which could mean that not enough coins were being bought or sold cumulatively. That could either point to the fact that people using this coin were not very confident with it's stability and hence the transaction amounts were short or this coin might not be considered or used for investment purpose.

















